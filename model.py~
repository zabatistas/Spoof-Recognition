from __future__ import division, print_function

import sys
import time
from datetime import datetime
import math
import tensorflow as tf
import numpy as np
from lib.model_io import save_variables
from lib.precision import _FLOATX


class CNN(train_params, train_params_value, valid_params,valid_params_value):

    batch_size = 256
    height = 64
    width = 17
    channels = 1

    def __init__(self, model_id=None,train_params,train_params_value,valid_params,valid_params_value):
        self.model_id = model_id
	self.train_params = train_params
	self.train_params_value = train_params_value
	self.valid_params = valid_params
	self.valid_params_value = valid_params_value

    def inference(self, X, reuse=tf.AUTO_REUSE, is_training=True):
        with tf.variable_scope("inference", reuse=reuse):
            # Implement your network here

            # Input Layer
            input_layer = tf.reshape(X, [-1, 64, 17, 1])

            # Convolutional Layer #1
            conv1 = tf.layers.conv2d(
                inputs=input_layer,
                filters=16,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Convolutional Layer #2
            conv2 = tf.layers.conv2d(
                inputs=conv1,
                filters=16,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Pooling Layer #1
            pool1 = tf.nn.max_pool(
                    inputs=conv2,
                    ksize = [1, 2, 1, 1],
                    strides=[1, 2, 1, 1],
                    padding='VALID')

            # Convolutional Layer #3
            conv3 = tf.layers.conv2d(
                inputs=pool1,
                filters=32,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Convolutional Layer #4
            conv4 = tf.layers.conv2d(
                inputs=conv3,
                filters=32,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            #Pooling Layer #2
            pool2 = tf.nn.max_pool(
                    inputs=conv4,
                    ksize = [1, 2, 1, 1],
                    strides=[1, 2, 1, 1],
                    padding='VALID')

            # Convolutional Layer #5
            conv5 = tf.layers.conv2d(
                inputs=pool2,
                filters=32,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Convolutional Layer #6
            conv6 = tf.layers.conv2d(
                inputs=conv5,
                filters=32,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            #Pooling Layer #3
            pool3 = tf.nn.max_pool(
                    inputs=conv6,
                    ksize = [1, 2, 2, 1],
                    strides=[1, 2, 2, 1],
                    padding='VALID')

            # Convolutional Layer #7
            conv7 = tf.layers.conv2d(
                inputs=pool3,
                filters=64,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Convolutional Layer #8
            conv8 = tf.layers.conv2d(
                inputs=conv7,
                filters=64,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Pooling Layer #4
            pool4 = tf.nn.max_pool(
                    inputs=conv8,
                    ksize = [1, 2, 2, 1],
                    strides=[1, 2, 2, 1],
                    padding='VALID')

            # Convolutional Layer #9
            conv9 = tf.layers.conv2d(
                inputs=pool4,
                filters=64,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            # Convolutional Layer #10
            conv10 = tf.layers.conv2d(
                inputs=conv9,
                filters=64,
                kernel_size=[3, 3],
                padding='SAME',
                activation=tf.nn.relu)

            #Pooling Layer #5
            pool5 = tf.nn.max_pool(
                    inputs=conv10,
                    ksize = [1, 2, 2, 1],
                    strides=[1, 2, 2, 1],
                    padding='VALID')


            # Dense Layer
            pool5_flat = tf.reshape(pool5, [-1, 2 * 2 * 64])
            dense = tf.layers.dense(inputs=pool5_flat, units=512, activation=tf.nn.relu)
            #dropout
            dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

            # Logits Layer
            logits = tf.layers.dense(inputs=dropout, units=2)
            Y=logits

        return Y

    def define_train_operations(self):


        # --- Train computations
        self.trainDataReader = trainDataReader

        X_data_train = tf.placeholder(ft.float32, shape=(self.batch_size,self.height,self.width,self.channels)) # Define this

        Y_data_train = tf.placeholder(ft.int32, shape=(self.batch_size)) # Define this

        Y_net_train = self.inference(X_data_train, reuse=False) # Network prediction

         # Loss of train data
        self.train_loss = tf.reduce_mean(tf.nn.sparce_softmax_cross_entropy_with_logits(labels=Y_data_train, logits=Y_net_train, name='train_loss'))

        # define learning rate decay method
        global_step = tf.Variable(0, trainable=False, name='global_step')
        learning_rate = 0.001 # Define it

        # define the optimization algorithm
        optimizer = tf.train.AdamOptimizer(learning_rate) # Define it

        trainable = tf.trainable_variables()
        self.update_ops = optimizer.minimize(self.train_loss, var_list=trainable, global_step=global_step)

        # --- Validation computations
        X_data_valid = tf.placeholder(ft.float32, shape=(self.batch_size,self.height,self.width,self.channels)) # Define this
        Y_data_valid = tf.placeholder(ft.int32, shape=(batch_size)) # Define this


        Y_net_valid = self.inference(X_data_valid, reuse=True) # Network prediction

        # Loss of validation data
        self.valid_loss = tf.reduce_mean(tf.nn.sparce_softmax_cross_entropy_with_logits(labels=Y_data_valid, logits=Y_net_valid, name='valid_loss'))



    def train_epoch(self, sess):
        train_loss = 0
        total_batches = 0

        while total_batches < len(self.train_params)/self.batch_size    #loop through train batches:
            mean_loss, ops = sess.run([self.train_loss, self.update_ops],
                                    feed_dict={X_data_train: self.train_params[total_batches*batch_size:total_batches*(batch_size+1)],
                                               Y_data_train: self.train_params_values[total_batches*batch_size:total_batches*(batch_size+1)]})
            if math.isnan(mean_loss):
                print('train cost is NaN')
                break
            train_loss += mean_loss
            total_batches += 1

        if total_batches > 0:
            train_loss /= total_batches

        return train_loss


    def valid_epoch(self, sess):
        valid_loss = 0
        total_batches = 0

        while total_batches<len(self.valid_params)/self.batch_size   # Loop through valid batches:
            mean_loss = sess.run(self.valid_loss,
                                 feed_dict={X_data_valid: self.valid_params[total_batches*batch_size:total_batches*(batch_size+1)],
                                            Y_data_valid: self.valid_params_value[total_batches*batch_size:total_batches*(batch_size+1)]})
            if math.isnan(mean_loss):
                print('valid cost is NaN')
                break
            valid_loss += mean_loss
            total_batches += 1

        if total_batches > 0:
            valid_loss /= total_batches

        return valid_loss


    def train(self, sess):
        start_time = time.clock()

        n_early_stop_epochs = 2# Define it
        n_epochs = 10# Define it

        saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=4)

        early_stop_counter = 0

        init_op = tf.group(tf.global_variables_initializer())

        sess.run(init_op)

        min_valid_loss = sys.float_info.max
        epoch = 0
        while (epoch < n_epochs):
            #shuffling data at every epoch
            index = np.arange(len(self.train_params))
            np.random.shuffle(index)
            self.train_params = self.train_params[index]
            self.train_params_values = self.train_params_values[index]

            index = np.arange(len(self.valid_params))
            np.random.shuffle(index)
            self.valid_params = self.valid_params[index]
            self.valid_params_values = self.valid_params_values[index]

            epoch += 1
            epoch_start_time = time.clock()

            train_loss = self.train_epoch(sess)
            valid_loss = self.valid_epoch(sess)

            epoch_end_time = time.clock()

            info_str = 'Epoch=' + str(epoch) + ', Train: ' + str(train_loss) + ', Valid: '
            info_str += str(valid_loss) + ', Time=' + str(epoch_end_time - epoch_start_time)
            print(info_str)

            if valid_loss < min_valid_loss:
                print('Best epoch=' + str(epoch))
                save_variables(sess, saver, epoch, self.model_id)
                min_valid_loss = valid_loss
                early_stop_counter = 0
            else:
                early_stop_counter += 1

            if early_stop_counter > n_early_stop_epochs:
                # too many consecutive epochs without surpassing the best model
                print('stopping early')
                break

        end_time = time.clock()
        print('Total time = ' + str(end_time - start_time))


    def define_predict_operations(self):
        self.X_data_test_placeholder = tf.placeholder(ft.float32, shape=(self.batch_size,self.height,self.width,self.channels))

        self.Y_net_test = self.inference(self.X_data_test_placeholder, reuse=False)


    def predict_utterance(self, sess, testDataReader, dataWriter):
        # Define it
